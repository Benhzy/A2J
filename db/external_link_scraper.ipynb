{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq #urllib will get the page itself\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "import requests\n",
    "import sys\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get('https://www.msf.gov.sg/Adoption/Pages/Who-can-Adopt.aspx', headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption_sg'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get('https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-citizen-or-PR.aspx', headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption-china'\n",
    "url = 'https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-child-from-the-Peoples-Republic-of-China.aspx'\n",
    "page_title = '\\n' + source + '_' + title + ' ' + url + '\\n'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption-foreign'\n",
    "url = 'https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-foreign-child-excluding-children-from-the-Peoples-Republic-of-China.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'adoption-of-children-act-1939'\n",
    "url = 'https://sso.agc.gov.sg/Act/ACA1939'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'family-justice-act-2014'\n",
    "url = 'https://sso.agc.gov.sg/Act/FJA2014'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'family-justice-courts'\n",
    "title = 'practice-directions-adoption'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2008-Adoption%20of%20Children.html'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['p'],{'id':'main-content'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "# print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'children-young-person-act'\n",
    "url = 'https://sso.agc.gov.sg/Act/CYPA1993'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'intoxicating-substances'\n",
    "url = 'https://sso.agc.gov.sg/Act/ISA1987'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "        file.write(page_title)\n",
    "        file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'penal-code-1871'\n",
    "url = 'https://sso.agc.gov.sg/Act/PC1871'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'muslim-marriage-divorce'\n",
    "url = 'https://sso.agc.gov.sg/SL/AMLA1966-R1'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'women-charter'\n",
    "url = 'https://sso.agc.gov.sg/Act/WC1961'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "familyjusticecourt_dissolution-marriage https://epd.familyjusticecourts.gov.sg/Part%2006-Proceedings%20for%20the%20Dissolution%20of%20Marriage%20under%20Part%20X%20of%20Women%E2%80%99s%20Charter.html\n"
     ]
    }
   ],
   "source": [
    "source = 'familyjusticecourt'\n",
    "title = 'dissolution-marriage'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2006-Proceedings%20for%20the%20Dissolution%20of%20Marriage%20under%20Part%20X%20of%20Women%E2%80%99s%20Charter.html'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "familyjusticecourt_sumonses https://epd.familyjusticecourts.gov.sg/Part%2013-General%20Procedure.html#76-summonses\n"
     ]
    }
   ],
   "source": [
    "source = 'familyjusticecourt'\n",
    "title = 'sumonses'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2013-General%20Procedure.html#76-summonses'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'guardianship-of-infants'\n",
    "url = 'https://sso.agc.gov.sg/Act/GIA1934#legis'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a045168fa5d23a35a016377cbad9cdf47ee7aa21ad8a22c65b158e0c876ea015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
