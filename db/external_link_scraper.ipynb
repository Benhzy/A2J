{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq #urllib will get the page itself\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "import requests\n",
    "import sys\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get('https://www.msf.gov.sg/Adoption/Pages/Who-can-Adopt.aspx', headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption_sg'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get('https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-citizen-or-PR.aspx', headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption-china'\n",
    "url = 'https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-child-from-the-Peoples-Republic-of-China.aspx'\n",
    "page_title = '\\n' + source + '_' + title + ' ' + url + '\\n'\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'adoption-foreign'\n",
    "url = 'https://www.msf.gov.sg/Adoption/Pages/How-to-adopt-a-foreign-child-excluding-children-from-the-Peoples-Republic-of-China.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'adoption-of-children-act-1939'\n",
    "url = 'https://sso.agc.gov.sg/Act/ACA1939'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'family-justice-act-2014'\n",
    "url = 'https://sso.agc.gov.sg/Act/FJA2014'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'family-justice-courts'\n",
    "title = 'practice-directions-adoption'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2008-Adoption%20of%20Children.html'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['p'],{'id':'main-content'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('adoption_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "# print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'children-young-person-act'\n",
    "url = 'https://sso.agc.gov.sg/Act/CYPA1993'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'intoxicating-substances'\n",
    "url = 'https://sso.agc.gov.sg/Act/ISA1987'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "        file.write(page_title)\n",
    "        file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'penal-code-1871'\n",
    "url = 'https://sso.agc.gov.sg/Act/PC1871'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('careprotect_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'muslim-marriage-divorce'\n",
    "url = 'https://sso.agc.gov.sg/SL/AMLA1966-R1'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'women-charter'\n",
    "url = 'https://sso.agc.gov.sg/Act/WC1961'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "familyjusticecourt_dissolution-marriage https://epd.familyjusticecourts.gov.sg/Part%2006-Proceedings%20for%20the%20Dissolution%20of%20Marriage%20under%20Part%20X%20of%20Women%E2%80%99s%20Charter.html\n"
     ]
    }
   ],
   "source": [
    "source = 'familyjusticecourt'\n",
    "title = 'dissolution-marriage'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2006-Proceedings%20for%20the%20Dissolution%20of%20Marriage%20under%20Part%20X%20of%20Women%E2%80%99s%20Charter.html'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "familyjusticecourt_sumonses https://epd.familyjusticecourts.gov.sg/Part%2013-General%20Procedure.html#76-summonses\n"
     ]
    }
   ],
   "source": [
    "source = 'familyjusticecourt'\n",
    "title = 'sumonses'\n",
    "url = 'https://epd.familyjusticecourts.gov.sg/Part%2013-General%20Procedure.html#76-summonses'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'guardianship-of-infants'\n",
    "url = 'https://sso.agc.gov.sg/Act/GIA1934#legis'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('divorce_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'mental-capacity act'\n",
    "url = 'https://sso.agc.gov.sg/Act/MCA2008'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div','table'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'lpa-intro'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/The-LPA-The-Lasting-Power-of-Attorney.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'lpa-cert-issuer'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/The-LPA-Where-to-find-a-Certificate-Issuer.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'lpa-usage'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/The-LPA-Using-a-Lasting-Power-of-Attorney.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'court-appointed-deputy-intro'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/What-Is-A-Court-Appointed-Deputy.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'court-appointed-deputy-procedure'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/How-Do-I-Apply-For-the-Appointment-of-a-Deputy.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'court-appointed-deputy-procedure'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/Deputyship.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'pdd-intro'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/About-PDD.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'pdd-eligibility'\n",
    "url = 'https://www.msf.gov.sg/opg/Pages/Registering-as-a-Professional-Deputy.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('deputyship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'enforcement-muslim-law-act'\n",
    "url = 'https://sso.agc.gov.sg/Act/AMLA1966?ProvIds=pr53-#pr53-'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'fragview'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'inheritance-act-1966'\n",
    "url = 'https://sso.agc.gov.sg/Act/IFPA1966'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'legitimacy-act-1934'\n",
    "url = 'https://sso.agc.gov.sg/Act/LA1934'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'maintenance-of-parents-1995'\n",
    "url = 'https://sso.agc.gov.sg/Act/MPA1995'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'maintenance-order-1975'\n",
    "url = 'https://sso.agc.gov.sg/Act/MOREA1975'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'probate-admin-1934'\n",
    "url = 'https://sso.agc.gov.sg/Act/PAA1934'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'scarta-1934'\n",
    "url = 'https://sso.agc.gov.sg/Act/SCARTA2013'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'vsa-1974'\n",
    "url = 'https://sso.agc.gov.sg/Act/VSA1974'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'protection-vulnerable-adult-p12'\n",
    "url = 'https://sso.agc.gov.sg/Act/VAA2018?ProvIds=P12-#P12-P21-'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'fragview'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('appeal_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'fgo'\n",
    "url = 'https://www.msf.gov.sg/policies/Children-and-Youth/Pages/Family-Guidance-Order.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('fgo_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'scja-1969-pr17'\n",
    "url = 'https://sso.agc.gov.sg/Act/SCJA1969#pr17-'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'fragview'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('guardianship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'family-justice-act-2014-pr22'\n",
    "url = 'https://sso.agc.gov.sg/Act/FJA2014#pr22-'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'fragview'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('guardianship_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'hcch'\n",
    "title = 'hcch'\n",
    "url = 'https://www.hcch.net/en/instruments/conventions/full-text/'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'id':'maincontent'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('international_child_abduction_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'sca'\n",
    "url = 'https://www.msf.gov.sg/Singapore-Central-Authority/Pages/default.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'content'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('international_child_abduction_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'mlaw'\n",
    "title = 'estate-other-assets'\n",
    "url = 'https://pto.mlaw.gov.sg/deceased-cpf-estate-monies/information-for-next-of-kin-estate-monies/'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'content'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('probate_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'wills-act-1838'\n",
    "url = 'https://sso.agc.gov.sg/Act/WA1838'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('probate_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'residential-property-act-1976'\n",
    "url = 'https://sso.agc.gov.sg/Act/RPA1976'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('probate_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'sso'\n",
    "title = 'interstate-succession-act-1976'\n",
    "url = 'https://sso.agc.gov.sg/Act/ISA1967'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'legis'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('probate_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'maintenance-act'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/Legislation.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'conciliation-process'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/ConciliationProcess.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'maintenance-application'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/HowtoApplyforMaintenance.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'maintenance-respond'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/HowtoRespondtoaNoticeofApplication.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "        file.write(page_title)\n",
    "        file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'mediation'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/Mediation.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'tribunal'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/TribunalHearing.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'msf'\n",
    "title = 'respond-notice-variation'\n",
    "url = 'https://www.msf.gov.sg/maintenanceofparents/Pages/HowtorespondtoaNoticeofVariation.aspx'\n",
    "page_title = '\\n\\n' + source + '_' + title + ' ' + url\n",
    "user_agent = {'User-agent' : 'Mozilla/5.0'}\n",
    "page = requests.get(url, headers=user_agent)\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "tr_detailed_desc = soup.find_all(['div'],{'class':'contentWithSide'})\n",
    "# print (tr_detailed_desc)\n",
    "for text in tr_detailed_desc:\n",
    "    var = text.get_text()\n",
    "    # print(var)\n",
    "    # with open('maintenance_external.txt','a', encoding=\"utf-8\") as file:\n",
    "    #     file.write(page_title)\n",
    "    #     file.write(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a045168fa5d23a35a016377cbad9cdf47ee7aa21ad8a22c65b158e0c876ea015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
